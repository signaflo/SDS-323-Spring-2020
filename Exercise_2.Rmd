---
title: "SDS 323 Exercises 2"
author: "Kyle Carter, Jacob Rachiele, Crystal Tse, Jinfang Yan"
date: "3/13/2020"
# output: HTML_document
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, cache = TRUE,autodep = TRUE ,cache.comments = FALSE, message = FALSE, warning = FALSE)
```

```{r setup, include = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(FNN)
library(mosaic)
library(foreach)
library(ggplot2)
library(scales)
library(kableExtra)
# sclass = read.csv("data/sclass.csv")
sclass = read.csv("sclass.csv")
set.seed(909)
```

## Problem 1: S Class

The Mercedes S Class is an unusual car model name that encompasses a broad range of cars with vastly different characteristics, making it tough to accurately predict pricing. 

For this analysis, only two trims were compared: 350 and 65 AMG. Since 65 AMG cars are among the higher horsepower offerings by Mercedes, they are much more expensive and not as many are sold, explaining why there are fewer observations than the 350 trim.
 
However, the 65 AMG trim was observed across more years, yielding a smoother scatterplot, whereas the 350 trim had a big gap in years, lending to a disjoint characteristic.

Below, the scatterplot with both trims is shown.

```{r echo=FALSE}


#subset into 350 and 65 AMG trim
sclass350 <- subset(sclass, trim == "350")
sclass65 <- subset(sclass, trim == "65 AMG")

# colors <- c("350 Trim" = "black"
#            , "65 AMG Trim" = "red")


# custom formatting function
scaleFUN <- function(x) sprintf("%.f", x/1000)

scale_dol_FUN <- function(x) sprintf("$%.f", x/1000)

ggplot() +
  labs(
    title = "Price vs Mileage for 350 and 65 AMG Trims",
    x = "Mileage",
    y = "Price",
    color = "Legend"
  ) +
  geom_point(data = sclass350, aes(x = mileage, y = price, color ="350"))+
  geom_point(data = sclass65, aes(x = mileage, y = price, color = "65 AMG"))+
  scale_x_continuous(name = "Mileage (thousands)", labels = scaleFUN)+
  scale_y_continuous(name = "Price (thousands of dollars)", labels = scale_dol_FUN)

# scale_colour_manual(
#   name = "Legend",
#   values = colors
# )
```


The 350 trim only has data on the years 1994, 1995, 2006, 2012, and 2013. Each year is concentrated in a particular section of the scatterplot, so it is likely that an observation's nearest neighbors would be close to it in time as well. Because of the huge gap in time between 1995 and 2006, there is a disjoint section for cars with mileage around 25,000. This may influence the model because of the jump in data.

```{r echo = FALSE}
ggplot(data=sclass350, aes(x=mileage, y=price)) +
  geom_point() +
  labs(title="Price vs Mileage for 350 Trim", x="Mileage", y="Price")+
  scale_x_continuous(name = "Mileage (thousands)", labels = scaleFUN)+
  scale_y_continuous(name = "Price (thousands of dollars)", labels = scale_dol_FUN)
  
```

Faceting by years for the 350 trim reveals that certain years have similar observations overall.

```{r echo = FALSE}


ggplot(data=sclass350, aes(x=mileage, y=price)) +
  geom_point() +
  labs(title="Price vs Mileage for 350 Trim", x="Mileage", y="Price")+
  facet_wrap(~year) +
  scale_x_continuous(name = "Mileage (thousands)", labels = scaleFUN)+
  scale_y_continuous(name = "Price(thousands of dollars)", labels= scale_dol_FUN)
```


In contrast, the 65 AMG trim, with much fewer total observations, has a smooth graph because the observations were gathered continuously for more years (2006 through 2013, and 2015).

```{r echo = FALSE}
# use the below command to see years in sclass65 dataframe
# sort((unique(sclass65$year)))
ggplot(data=sclass65, aes(x=mileage, y=price)) +
  geom_point() +
  labs(title="Price vs Mileage for 65 AMG Trim", x="Mileage", y="Price")+
  scale_x_continuous(name = "Mileage (thousands)", labels = scaleFUN)+
  scale_y_continuous(name = "Price (thousands of dollars)", labels = scale_dol_FUN)
```

Faceting by year for the 65 AMG trim also reveals clusters of observations for each year.

```{r echo = FALSE}
ggplot(data=sclass65, aes(x=mileage, y=price)) +
  geom_point() +
  labs(title="Price vs Mileage Across Years for 65 AMG Trim", x="Mileage", y="Price") +
  facet_wrap(~year)+
  scale_x_continuous(name = "Mileage (thousands)", labels = scaleFUN)+
  scale_y_continuous(name = "Price (thousands of dollars)", labels = scale_dol_FUN)
```


We start with the 350 trim model and find the optimal K that minimizes the RMSE after iterating through many train-test splits. Then we compare the KNN model to linear regression models, one of which predicts price using mileage, and the other uses a polynomial of mileage predicting price. The red line is the RMSE for the linear regression model and the blue line is the second-degree polynomial.


```{r echo=FALSE}
# Train-test split for sclass 350
N_350 = nrow(sclass350)
N_350train = round(0.8*N_350)
N_350test = N_350 - N_350train


train_350ind = sample.int(N_350, N_350train, replace=FALSE)

# Define the training and testing set
D_350train = sclass350[train_350ind,]
D_350test = sclass350[-train_350ind,]

D_350test = arrange(D_350test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
x_350train = data.frame(mileage=D_350train$mileage)
y_350train = D_350train$price
x_350test = data.frame(mileage=D_350test$mileage)
y_350test = D_350test$price

#linear and quadratic models
lm1_350 = lm(price ~ mileage, data=D_350train)
lm2_350 = lm(price ~ poly(mileage, 2), data=D_350train)

#Define RMSE function
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

#find the best K for the 350 trim
x350=dplyr::select(sclass350, mileage)
y350=sclass350$price
n350=length(y350)

n350_train=round(0.8*n350)
n350_test=n350-n350_train
k_grid = seq(1, 50, by=1)
rmse_grid_out350 = foreach(k = k_grid,  .combine='c') %do% {
  out350 = do(5000)*{
    train_ind = sample.int(n350, n350_train)
    X_train350 = x350[train_ind,]
    X_test350 = x350[-train_ind,]
    y_train350 = y350[train_ind]
    y_test350 = y350[-train_ind]
    
    knn_mod350 = FNN::knn.reg(as.data.frame(X_train350), as.data.frame(X_test350), y_train350, k=k)
    
    rmse(y_test350, knn_mod350$pred)
  } 
  mean(out350$result)
}
```


```{r echo=FALSE}
y_pred350_1 = predict(lm1_350, D_350test)
lm1_350rmse = rmse(D_350test$price, y_pred350_1)
y_pred350_2 = predict(lm2_350, D_350test)
lm2_350rmse = rmse(D_350test$price, y_pred350_2)

rmse_grid_out350 = data.frame(K = k_grid, RMSE = rmse_grid_out350)

ind_best350 = which.min(rmse_grid_out350$RMSE)
k_best350 = k_grid[ind_best350]

g1 <- data.frame(k_best350, minrmse350=min(rmse_grid_out350$RMSE))

p_out = ggplot(data=rmse_grid_out350) + 
  geom_path(aes(x=K, y=RMSE), color="violet", size=1.5) + 
  geom_hline(yintercept=lm2_350rmse, color='blue', size=1) +
  geom_hline(yintercept=lm1_350rmse, color='red', size=1) +
  geom_point(data=g1, aes(k_best350, y=minrmse350), color="black", size=3) +
  geom_text(data=g1, aes(x=k_best350, y=minrmse350, label=k_best350), vjust=0.5, hjust=-0.5, size=4) +
  labs(title="Root Mean Squared Error vs K for 350 Trim Regression and KNN Models", 
       x="K", y="Root Mean Squared Error (RMSE)")

p_out
```


```{r echo=FALSE}
#fitting the KNN Model
train_350ind = sort(sample.int(N_350, N_350train, replace=FALSE))
D_350train = sclass350[train_ind,] 
D_350train = arrange(D_350train, mileage)
y_train350 = D_350train$price
X_train350 = data.frame(mileage=jitter(D_350train$mileage))

knn350 = FNN::knn.reg(X_train350, X_train350, y_train350, k = k_best350)

subtitle350 = paste("Optimal K =", k_best350)
D_350train$ypred = knn350$pred
p_train = ggplot(data = D_350train) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey')
p_train + geom_path(mapping = aes(x=mileage, y=ypred), color='red', size=1.5) +
  labs(title="KNN model for 350 Trim", subtitle=subtitle350, x="Mileage", y="Price")

```

Repeat the procedure for the 65 AMG trim.
<!-- Train-test split for sclass 65 -->

```{r echo=FALSE}
N_65 = nrow(sclass65)
N_65train = floor(0.8*N_65)
N_65test = N_65 - N_65train


train_65ind = sample.int(N_65, N_65train, replace=FALSE)

# Define the training and testing set
D_65train = sclass65[train_65ind,]
D_65test = sclass65[-train_65ind,]

D_65test = arrange(D_65test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
x_65train = data.frame(mileage=D_65train$mileage)
y_65train = D_65train$price
x_65test = data.frame(mileage=D_65test$mileage)
y_65test = D_65test$price

#linear and quadratic models
lm1_65 = lm(price ~ mileage, data=D_65train)
lm2_65 = lm(price ~ poly(mileage, 2), data=D_65train)

# define RMSE function
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# find best K for the 65 subset
x65=dplyr::select(sclass65, mileage)
y65=sclass65$price
n65=length(y65)

n65_train=round(0.8*n65)
n65_test=n65-n65_train
k_grid65 = seq(1, 50, by=1)
rmse_grid_out65 = foreach(k = k_grid65,  .combine='c') %do% {
  out65 = do(5000)*{
    train_ind = sample.int(n65, n65_train)
    X_train65 = x65[train_ind,]
    X_test65 = x65[-train_ind,]
    y_train65 = y65[train_ind]
    y_test65 = y65[-train_ind]
    
    knn_mod65 = FNN::knn.reg(as.data.frame(X_train65), as.data.frame(X_test65), y_train65, k=k)
    
    rmse(y_test65, knn_mod65$pred)
  } 
  mean(out65$result)
}

y_pred65_1 = predict(lm1_65, D_65test)
lm1_65rmse = rmse(D_65test$price, y_pred65_1)
y_pred65_2 = predict(lm2_65, D_65test)
lm2_65rmse = rmse(D_65test$price, y_pred65_2)

rmse_grid_out65 = data.frame(K = k_grid65, RMSE = rmse_grid_out65)

ind_best65 = which.min(rmse_grid_out65$RMSE)
k_best65 = k_grid65[ind_best65]

g2 <- data.frame(k_best65, minrmse65=min(rmse_grid_out65$RMSE))

g_out = ggplot(data=rmse_grid_out65) + 
  geom_path(aes(x=K, y=RMSE), color="violet", size=1.5) + 
  geom_hline(yintercept=lm2_65rmse, color='blue', size=1) +
  geom_hline(yintercept=lm1_65rmse, color='red', size=1) +
  geom_point(data=g2, aes(k_best65, y=minrmse65), color="black", size=3) +
  geom_text(data=g2, aes(x=k_best65, y=minrmse65, label=k_best65), vjust=0.5, hjust=-0.5, size=4) +
  labs(title="RMSE vs K for 65 Trim Regression and KNN Models", 
       x="K", y="Root Mean Squared Error (RMSE)")

g_out
```

```{r echo=FALSE}
#fitting the KNN Model
train_65ind = sort(sample.int(N_65, N_65train, replace=FALSE))
D_65train = sclass65[train_65ind,] 
D_65train = arrange(D_65train, mileage)
y_train65 = D_65train$price
X_train65 = data.frame(mileage=jitter(D_65train$mileage))

knn65 = FNN::knn.reg(X_train65, X_train65, y_train65, k = k_best65)

subtitle65 = paste("Optimal K =", k_best65)
D_65train$ypred = knn65$pred
g_train = ggplot(data = D_65train) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey')
g_train + geom_path(mapping = aes(x=mileage, y=ypred), color='red', size=1.5) +
  labs(title="KNN Model for 65 AMG Trim", subtitle= subtitle65, x="Mileage", y="Price")
```

### Why do the optimal Ks differ for each trim?

Looking at a summary of price for the two different trims, it is apparent that there is a tremendous difference in the price ranges for the two car sub-models. The distribution of mileage for the two trims is also different, with the 350 trim having a more normal distribution with positive skew, and the 65 trim having many observations with very low or zero values.

```{r echo=FALSE}

sub65vs350 = sclass[sclass$trim %in% c("65 AMG", "350"),]

plot.data <- rbind(sclass350, sclass65)
ggplot(plot.data, aes(x = trim, y = price)) + geom_boxplot() + labs(title = "Significant Price Range Differences for Trims", x = "Trim", y = "Price")
```


```{r echo = FALSE}
# favstats(~price, data=sclass350)%>% kable(caption = "Summary Statistics for 350 Trim") %>% kable_styling()
```

```{r echo=FALSE}
# favstats(~price, data=sclass65) %>% kable(caption = "Summary Statistics for 65 AMG Trim") %>% kable_styling()

```

```{r echo = FALSE}
ggplot(sclass350, aes(x=mileage))+ 
  geom_histogram(aes(y=..density..), colour="black", fill="white", bins=30)+
  geom_density(alpha=.2, fill="#FF6666") +
  ggtitle("Distribution of Mileage for 350 Trim")+
  labs(x="Mileage", y="Density")
```

```{r echo = FALSE}
ggplot(sclass65, aes(x=mileage))+ 
  geom_histogram(aes(y=..density..), colour="black", fill="white", bins=30)+
  geom_density(alpha=.2, fill="#FF6666") +
  ggtitle("Distribution of Mileage for 65 AMG Trim")+
  labs(x="Mileage", y="Density")
```

## Conclusion
For the 65 trim, the KNN model with the lowest RMSE seems to use K = `r k_best65` while the model for the 350 trim uses K = `r best350`. It seems that the 65 AMG trim has a much wider range, so the best KNN model generalizes over that variation. In contrast, the 350 trim, although its mean is being pulled downwards from low values, is more normally distributed and has a tighter distribution. Also, there are fewer observations for the 65 AMG trim, so it is more prone to outliers. Thus, due to the noise and few points to average over, the model must be more flexible.

Visually, if we compare the price of each trim to mileage in the initial graphs, the 65 AMG trim points are more spread out and have several points with mileage values between 200,000 and 250,000 with low or zero prices that could skew the results. Since the trend here is much less obvious, the model benefits from a higher K; more points are being averaged over and it results in a more "smoothed out" model. Conversely, the 350 trim (although it appears to have 2 or 3 separate sub-groupings with different slopes) has a more linear trend, so the K performs better when it is smaller and more granular.

If we compare the RMSEs for both, the out-of-sample RMSE for the 65 trim model is almost twice that of the 350 model, so the model is worse for the 65 AMG trim, likely due to the variation as described above. The relatively higher K and RMSE values for the 65 AMG trim could suggest high bias; the model is oversimplifying and struggling to make a prediction. Another indication of this is that the regression models perform better for the 350 trim than for the 65 AMG trim; a second-degree polynomial does not perform much better than a linear line for the 350 trim, whereas the linear model has an extremely high error rate compared to the polynomial and the KNN model.

In short, the 65 AMG trim is more difficult to predict than the 350 trim due to less data being available, the more nonlinear pattern of price to mileage, and the presence of leverage outliers.

## Problem 2: Saratoga House Prices

### Question

#### What is the best price-modeling strategy for predicting property market value?

### Data

The data set used to answer this question includes the prices of 1,728 houses in 
Saratoga County, New York in the year 2006. In addition to price, the data set includes
15 other attributes for each house --- 9 quantitative, e.g., house age, living area,
and number of bedrooms, and 6 categorical, e.g., whether the house has central air
conditioning, is a new construction, the type of heating system, and the type of fuel
used for that heating system.

### Methods

Two methods were used to analyze the data and form predicted house prices. The first was
linear regression, which assumes that house price is a linear function of the predictive
attributes, and the second is K-nearest-neighbors regression, which makes no assumptions
about the form of the function used to predict prices.

Instead, it predicts the price of a house based on the prices of other houses that are most
similar to the one being considered, where similarity is measured by how close the values
of the predictor attributes are to the house being considered. For example, if square
footage was the only predictor attribute and the house whose price we want to predict was
3,000 square feet, then we would find the average price of the K houses that are closest to
being 3,000 square feet and use that as our prediction.

80% of the houses were selected at random to train each model. The remaining 20% of the
houses were used to test the predictive performance of each model, measured by the root
mean squared error (RMSE) obtained my comparing each model's predicted prices with the actual prices contained in the test set.

#### Attribute Selection

The attributes used as predictors of housing prices were determined by examining the
correlation structure of the data, by visual examination of the relationships among
the predictors and prices, and ultimately by the ability of each potential predictor to reduce the RMSE of the predictions.

The attributes that were used to predict housing prices are given in Table 1

__Table 1__

| Attribute        | Description                                            |
| ---------        | --------------------------------------                 |
| living area      | living area in square feet                             |
| bedrooms         | The number of bedrooms in the house                    |
| bathrooms        | The number of bathrooms in the house                   |
| central air      | whether the house includes central air conditioning    |
| age              | The age of the house in years                          |
| percent college  | The percent of the neighborhood that graduated college |
| waterfront       | Whether the house is a waterfront property             |
| new construction | Whether the house was newly constructed                |

### Results

```{r include = FALSE}
# Load required libraries and define rmse function.
library(tidyverse)
library(mosaic)
library(splines)
library(FNN)
library(foreach)
data(SaratogaHouses)

rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

# Simple wrapper function to test for features that aren't factors.
# not.factor <- function(x) !is.factor(x)

# Show the correlation matrix to get an idea of price effects and interactions.
# cor(select_if(SaratogaHouses, not.factor))
```

```{r}
n <- nrow(SaratogaHouses)
n_train <- round(0.8*n)  # round to nearest integer
n_test <- n - n_train

set.seed(550)
rmse_vals <- do(250) * {
  train_cases <- sample.int(n, n_train, replace=FALSE)
  test_cases <- setdiff(1:n, train_cases)
  saratoga_train <- SaratogaHouses[train_cases,]
  saratoga_test <- SaratogaHouses[test_cases,]
  
  model <- lm(price ~
                livingArea * centralAir +
                livingArea * bathrooms +
                livingArea * bedrooms +
                waterfront + newConstruction +
                bs(age, df = 7) + bs(pctCollege, df = 7),
              data = saratoga_train)
  
  yhat_test <- predict(model, saratoga_test)
  
  rmse(saratoga_test$price, yhat_test)
  
}

k_grid <- exp(seq(log(1), log(100), length=33)) %>% round %>% unique

rmse_grid <- foreach(K = k_grid, .combine='c') %do% {
  out <- do(100) * {
    
    train_cases <- sample.int(n, n_train, replace=FALSE)
    test_cases <- setdiff(1:n, train_cases)
    saratoga_train <- SaratogaHouses[train_cases,]
    saratoga_test <- SaratogaHouses[test_cases,]
    
    training_features <- model.matrix(~ livingArea + centralAir +
                                        bathrooms + bedrooms +
                                        waterfront + newConstruction +
                                        pctCollege + age - 1,
                                      data = saratoga_train)
    test_features <- model.matrix(~ livingArea + centralAir +
                                    bathrooms + bedrooms +
                                    waterfront + newConstruction +
                                    pctCollege + age - 1,
                                  data = saratoga_test)
    
    training_response <- saratoga_train$price
    test_response <- saratoga_test$price
    
    training_scale <- apply(training_features, 2, sd)
    
    training_features <- scale(training_features, scale = training_scale)
    test_features <- scale(test_features, scale = training_scale)
    
    knn_model <- knn.reg(training_features, test_features, training_response, k = K)
    rmse(test_response, knn_model$pred)
  }
  mean(out$result)
}

lm_rmse <- mean(rmse_vals$result) %>% round
rmse_results <- data.frame(k = k_grid, k_rmse = rmse_grid, lm_rmse = lm_rmse)
best_k_index <- which.min(rmse_results$k_rmse)
best_k <- rmse_results$k[best_k_index]
best_rmse <- rmse_results$k_rmse[best_k_index] %>% round
```

The best performing K-nearest-neighbors model was at K = `r best_k` which had a prediction
RMSE of `r as.integer(best_rmse)`.

On the other hand, the linear regression model achieved a far superior prediction RMSE of 
`r as.integer(lm_rmse)`.

Figure 1 displays prediction RMSE against increasing values of K. The RMSE for 
K-nearest-neighbors is displayed in red. The RMSE of the linear regression model is 
the blue horizontal line.

```{r}
best_k_point <- data.frame(best_k = best_k, best_rmse = best_rmse)
rmse_subtitle <- "RMSE for KNN and Linear Regression Models"
lm_text <- "RMSE for linear regression model"
best_k_label <- paste("K =", best_k)

rmse_plot <- ggplot(data = rmse_results)
rmse_plot + geom_path(aes(x = k, y = k_rmse), color = "red", size = 0.75, alpha = 0.5) +
  geom_hline(yintercept = lm_rmse, color = "blue", size = 0.75, alpha = 0.5) +
  ggtitle("Figure 1", subtitle = rmse_subtitle) +
  xlab("K") +
  ylab("RMSE (Root Mean Squared Error)") +
  geom_point(data = best_k_point, aes(x = best_k, y = best_rmse),
             color = "black", alpha = 0.75, size = 2) +
  geom_text(data = best_k_point, aes(x = best_k, y = best_rmse, label = best_k_label),
            vjust = 1, hjust = -0.25, size = 3.5) +
  geom_text(data = rmse_results, aes(x = floor(max(k) / 2), y = lm_rmse,
            label = lm_text), vjust = -1, size = 4)
  
```

The greater predictive accuracy of the linear regresson model is the result of using the
right set of predictor attributes, modeling the interactions among those predictors, and
adding flexibility to the model through the use of the cubic spline, which divides the
range of the predictor variable into distinct intervals and fits a cubic polynomial curve
to each interval separately.

In particular, using age and percent of college graduates as simple predictors doesn't 
give the linear model much improvement in predictive power. However, using the cubic spline
mentioned above results in a dramatic improvement. Figures 2 and 3 below show the result
of fitting a cubic spline separately for both.

```{r}
age_model <- lm(price ~ bs(age, df = 7), data = SaratogaHouses)
age_subtitle <- "price vs age fitted with cubic spline"
xlabel <- "age of house (in years)"
p <- ggplot(data = SaratogaHouses)
p + geom_point(aes(x = age, y = price), color = "gray", alpha = 0.5) +
  geom_line(aes(age, age_model$fitted.values), color = "red", alpha = 0.75, size = 0.5) +
  ggtitle("Figure 2", subtitle = age_subtitle) +
  xlab(xlabel)

college_model <- lm(price ~ bs(pctCollege, df = 7), data = SaratogaHouses)
college_subtitle <- "price vs percent college graduates fitted with cubic spline"
xlabel <- "percentage of college graduates in neighborhood"
p +  geom_point(aes(x = pctCollege, y = price), color = "gray", alpha = 0.5) +
  geom_line(aes(pctCollege, college_model$fitted.values), 
            color = "red", alpha = 0.75, size= 0.5) +
  ggtitle("Figure 3", subtitle = college_subtitle) +
  xlab(xlabel)
```

Including interactions in the linear regression model provides a way to incorporate the
effect that one predictor variable has on another predictor variable into the model.
For example, living area is a powerful predictor of housing prices considered alone.
But by also considering the effect that living area has on prices in the context of the
values of other predictor variables, more accurate modeling and hence predictions are made.

For example, Figure 4 shows not only that larger living areas are associated
with higher house prices, but that, for houses with similar living areas, the presence of
central air conditioning may have a substantial effect on the house price. Considering
the joint effect of living area with both the number of bedrooms and the number of
bathrooms also boosts the predictive accuracy of the model.

```{r}
fig4_subtitle <- "price vs living area and central air conditioning"
p + geom_point(aes(x = livingArea, y = price, color = centralAir)) +
  ggtitle("Figure 4", subtitle = fig4_subtitle) +
  xlab("living area") +
  labs(color = "Central Air")
```

Finally, as might be expected, including indicators for whether a house is a waterfront
property or is newly constructed also improves prediction accuracy. Figures 5 and 6 show
the relationship between each of those attributes and house prices.

```{r}
waterfront_subtitle <- "price distribution by whether house is on waterfront"
  p + geom_boxplot(aes(x = waterfront, y = price)) +
    ggtitle("Figure 5", subtitle = waterfront_subtitle) +
    xlab("is house a waterfront property?")

new_construction_subtitle <- "price distribution by whether house was newly constructed"
  p + geom_boxplot(aes(x = newConstruction, y = price)) +
    ggtitle("Figure 6", subtitle = new_construction_subtitle) +
    xlab("was house newly constructed?")
```

### Conclusion

The price of a house depends on many factors and relationships among those factors. Therefore a flexible modeling approach is necessary to give the most accurate predictions.

K-nearest-neighbors is considered a more flexible modeling strategy than linear
regression since it makes no assumptions about the underlying form of the relationship
between the response variable and the predictor variables, while linear regression does 
assume that a linear function can adequately model the response.

However, linear regression has two big advantages. One, it can be made substantially more
flexible by extending it with nonlinear techniques such as the cubic splines used on age
and percent of college graduates. Two, the interactions, i.e., context-specific effects
between variables, can be made clear and explicit. For example, linear regression gives a 
precise way to model how central air conditioning's effect on house price depends on
the living area of the house.

With that said, if K-nearest-neighbors had outperformed linear regression in terms of
predictive accuracy, then that would be the recommended approach for determining house 
prices and setting tax rates. But that was not the case here. In fact, the linear
regression model signficantally outperformed K-nearest-neighbors and therefore is the
recommended approach for predicting house prices to help determine tax rates in Saratoga
County.

Specifically, the Saratoga County tax authority should predict prices using a linear
regression model using the following approach.

1. Include living area and consider it the most important determinant of house price.
2. Consider central air conditioning, the number of bedrooms, the number of bathrooms,
and make sure to include the interaction between each of those predictors and living area.
3. Include waterfront property and new constructions as simple predictors.
4. Include age of the house and percent of college graduates in the neighborhood as 
predictors. However, use cubic splines with each predictor. More specifically, divide 
each predictor into 5 intervals using the 20th, 40th, 60th, and 80th percentile. Then, on
each interval fit a cubic polynomial and join together the resulting curves across all
intervals to achieve one smooth, non-linear curve.
 
By following the approach outlined above the Saratoga County tax authority will obtain
very accurate predictions for house prices and be able to confidentally determine the
appropriate taxing strategy.
```{r}
library(mosaic)
library(tidyverse)
library(FNN)
library(class)
library(caret)
library(foreach)
library(ggplot2)

library(kableExtra)
library(knitr)
library(ggthemes)


library(mosaic)
# library(doMC)

set.seed(1001)
articles = read.csv("online_news.csv")
articles <- articles %>% mutate(viral = ifelse(shares > 1400, 1, 0))
```
# Problem 3
## Becoming "Viral"
Mashable currently has about 49% of its articles go "viral," or get shared more than 1,400 times. Is there any way to improve this, or is it completely random?

There are several factors that can be considered. These include, but are not limited to, the subject matter (e.g., "Entertainment", "World", "Tech"), day of publication, and polarity of the words in the article's content and title.
 
## KNN Regression Approach
We fit a linear regression model to predict the number of times an article was shared using several variables, including the number of words in the title, number of links (in general and to other Mashable articles), subject matter, and average negative polarity of the article. These variables were selected based upon their statistical significance in a regression with all of the variables in the dataset used to predict number of shares.

A comprehensive overview of the variables used in our regression model is summarized in the table below.

__Table 1__

| Variable Name                 | Description                                             |
|-------------------------------|---------------------------------------------------------|
| n_tokens_title                | Number of words in the title                            |
| num_hrefs                     | Number of links                                         |
| num_self_hrefs                | Number of links to other articles published by Mashable |
| num_imgs                      | Number of images                                        |
| average_token_length          | Average length of the words in the content              |
| num_keywords                  | Number of keywords in the metadata                      |
| data_channel_is_lifestyle     | Is data channel 'Lifestyle'?                            |
| data_channel_is_entertainment | Is data channel 'Entertainment'?                        |
| data_channel_is_bus           | Is data channel 'Business'?                             |
| data_channel_is_socmed        | Is data channel 'Social Media'?                         |
| data_channel_is_tech          | Is data channel 'Tech'?                                 |
| data_channel_is_world         | Is data channel 'World'?                                |
| self_reference_min_shares     | Min. shares of referenced articles in Mashable          |
| avg_negative_polarity         | Avg. polarity of negative words                         |


These variables that help in explaining the number of times an article was shared also have an intuitive reasoning behind their inclusion in the model. For example, the number of words in the title might contribute to how "click-worthy" a certain article is, increasing the likelihood that is shared more frequently. Links to an article allow for easy access to its contents, and more keywords in the metadata make an article more likely to found by individuals that then share it.
```{r}
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```
Below are the different values of K tested to find the optimal K for the out-of-sample accuracy.

```{r}

# confusion matrix - make a table of KNN (regular, not classification) errors
k_grid <- exp(seq(log(1), log(100), length=30)) %>% round %>% unique
k_grid <- k_grid[k_grid != 2]
k_grid

```

Iterating across the above range of K to find the lowest RMSE (Root Mean Square Error), we try to find the best K.
```{r}

X = select(articles, n_tokens_title, 
               num_hrefs, num_self_hrefs, num_imgs, average_token_length,
               num_keywords, data_channel_is_lifestyle, data_channel_is_entertainment,
               data_channel_is_bus, data_channel_is_socmed,
               data_channel_is_tech, data_channel_is_world,
               self_reference_min_shares , avg_negative_polarity)
y = articles$shares
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train

# create a confusion matrix for multiple K values
# Make a train - test split 
repetitions <- 30
output_values = foreach(k = k_grid, .combine='rbind') %do% {
  out = do(repetitions)*{
    train_ind = sample.int(n, n_train)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    
    X_train <- model.matrix(~ n_tokens_title +
               num_hrefs + num_self_hrefs + num_imgs + average_token_length +
               num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
               data_channel_is_bus + data_channel_is_socmed +
               data_channel_is_tech + data_channel_is_world +
               self_reference_min_shares  + avg_negative_polarity - 1, data = X_train)
    X_test <-  model.matrix(~ n_tokens_title + 
               num_hrefs + num_self_hrefs + num_imgs + average_token_length +
               num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
               data_channel_is_bus + data_channel_is_socmed +
               data_channel_is_tech + data_channel_is_world +
               self_reference_min_shares  + avg_negative_polarity - 1, data = X_test)
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    knn_model = FNN::knn.reg(X_train_sc, X_test_sc, y_train, k=k)
    knn_model_predictions = knn_model$pred
    
    # out of sample confusion matrix
    viral_prediction = ifelse(knn_model_predictions > 1400, 1, 0)
    actual_viral <- ifelse(y_test > 1400, 1, 0)
    
    confusion_matrix = table(y = actual_viral, yhat = viral_prediction)
    output <- c(rmse = rmse(y_test, knn_model$pred))
    if (isTRUE(all.equal(dim(confusion_matrix), c(2, 2)))) {
      output <- c(output, correct_negatives = confusion_matrix[1,1],
                 correct_positives = confusion_matrix[2,2], 
                 wrong_negatives = confusion_matrix[2,1], 
                 wrong_positives = confusion_matrix[1,2])
    }
    output
  } 
  colMeans(out)
}
rownames(output_values) <- paste("K", k_grid, sep = "")

rmse_avg <- output_values[,"rmse"]
confusion_matrix_values <- output_values[,2:5]
best_rmse_k = k_grid[which.min(rmse_avg)]
best_rmse_k

rmse_best_k = min(rmse_avg)
rmse_best_k
# rmse_grid = data.frame(K = k_grid, RMSE = rmse_grid)
# ind_best = which.min(rmse_grid$RMSE)
# best_k = k_grid[ind_best]
# best_k
```
We find that the lowest RMSE is when K = `r best_rmse_k`. However, the K value with the highest out-of-sample accuracy has yet to be calculated.

```{r  }
accuracy <- rowSums(confusion_matrix_values[,1:2]) / rowSums(confusion_matrix_values)
best_acc_k = k_grid[which.max(accuracy)]
best_acc_k

rmse_most_accurate_k = rmse_avg[which(k_grid == best_acc_k)]
rmse_most_accurate_k

```
Above is the best K value for out-of-sample accuracy. Although it seems like a low K value is not optimal for minimizing RMSE since it would lead to overfitting of the data, K = `r best_acc_k` has a RMSE of `r rmse_most_accurate_k`, as opposed to K = `r best_rmse_k` with a RMSE of `r rmse_best_k`.

Below is a plot of the K against the out-of-sample accuracy rate. While the RMSE is lower for K = `r best_rmse_k`, creating a model that is applicable to new data and accurate is also a high priority.
```{r  }
plot(
  k_grid,
  accuracy,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "Accuracy Rate",
  main = "KNN Regression Accuracy",
  log = 'x'
)
```


```{r  }


plot(
  k_grid,
  rmse_avg,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "RMSE",
  main = "KNN Regression RMSE",
  log = 'x'
)

```



```{r  }
knn_model = knn.reg(train = X_train, test = X_test, y = y_train, k=best_acc_k)
knn_model_predictions <- knn_model$pred


yhat_test_viral1 = ifelse(knn_model_predictions > 1400,1, 0)
```

The confusion matrix for K = `r best_acc_k` is shown below.
```{r  }
confusion_out = table(knn.pred = yhat_test_viral1, Actual = articles[-train_ind, ]$viral) 

# Matrix
colnames(confusion_out) <- c("Not Viral","Viral")
rownames(confusion_out) <- c("Not Viral","Viral")
confusion_out %>% kable() %>% kable_styling()
```

The out-of-sample accuracy is shown below.
```{r  }
acc=sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy
acc
```

The overall error rate is shown below.
```{r  }
# overall error rate
err=1 - sum(diag(confusion_out))/sum(confusion_out)
err
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r  }
TP = confusion_out[2,2] / sum(confusion_out[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r  }
# The true negative rate is --%, which means --% percentage of Nonviral that are correctly identified.
TN = confusion_out[1,1] / sum(confusion_out[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r  }
# There are --% of Viral incorrectly identified as Nonviral 
FP = confusion_out[2,1] / sum(confusion_out[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r  }
# There are --% of NonViral incorrectly identified as Viral 
FN = confusion_out[1,2] / sum(confusion_out[,2])
FN
```
It is best to have both a low RMSE and high accuracy rate. 

K = `r best_rmse_k` has an average accuracy rate of about 0.4934416, error rate of 0.5065584, true positive rate of 0.9907716, and false positive rate of 0.9764151. 

Meanwhile, K = `r best_acc_k` has accuracy rate of `r acc`, error rate of `r err`, true positive rate of `r TP`, and false positive rate of `r FP`.

Thus, K =  satisfies the criteria better than K = `r best_rmse_k`, so we find the optimal K based on a higher accuracy rate out-of-sample.

In contrast, the null model, which always predicts that an article will not be viral, has a true positive rate of 0.4934416. The above KNN regression model has a slightly higher out-of-sample accuracy rate of about 0.552; however, it is inherently difficult to predict whether an article will go viral, so this number is still relatively low.

```{r  }
# null model, always predicts "not viral"
lm_lame = lm(!viral ~ 1, data = articles)

# do the TP for null
1 - coef(lm_lame)
```
## KNN Classification Approach

Approaching the problem considering the dependent variable as a binary variable, rather than a numerical variable, yields different results. We defined a dummy variable "Viral" that equals 1 when the number of shares is greater than 1400. 

```{r  }

# calc_class_err = function(actual, predicted) {
#   mean(actual != predicted)
# }

calc_class_err = function(actual, predicted, n_testing) {
  mean((sum(actual != predicted))/n_testing)
}

y = articles$viral
N = nrow(articles)
N_train2 = floor(0.8 * N)
N_test2 = N - N_train2
    
    
k_to_try = k_grid
# k_to_try = list(3,4,7)
reps = 5

output_values_2 = foreach(k = k_to_try, .combine = 'rbind') %do% {
  out2 = do(reps) * {
    train_ind2 = sample.int(N, N_train2, replace = FALSE)
    # D_train2 = articles[train_ind2, ]
    # D_test2 = articles[-train_ind2, ]
    
    X_train2 = X[train_ind2,]
    X_test2 = X[-train_ind2,]
    y_train2 = y[train_ind2]
    y_test2 = y[-train_ind2]
    
    X_train2 = model.matrix(
      ~
        n_tokens_title +
        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
        num_keywords + data_channel_is_lifestyle +
        data_channel_is_entertainment +
        data_channel_is_bus + data_channel_is_socmed +
        data_channel_is_tech + data_channel_is_world +
        self_reference_min_shares  + avg_negative_polarity - 1,
      data = X_train2
    )
    
    X_test2 = model.matrix(
      ~
        n_tokens_title +
        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
        num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
        data_channel_is_bus + data_channel_is_socmed +
        data_channel_is_tech + data_channel_is_world +
        self_reference_min_shares  + avg_negative_polarity - 1,
      data = X_test2
    )
    
    
    # scale the training set features
    scale_factors = apply(X_train2, 2, sd)
    X_train_sc2 = scale(X_train2, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc2 = scale(X_test2, scale=scale_factors)
   
    knn_model_2 = class::knn(train = X_train_sc2,
               test  = X_test_sc2,
               cl    = y_train2,
               k     = k)
    
     # out of sample confusion matrix
    viral_prediction_2 = ifelse(knn_model_2 == 1, 1, 0)
    actual_viral <- ifelse(y_test2 == 1, 1, 0)
    
    confusion_matrix2 = table(y = actual_viral, yhat = viral_prediction_2)
    output2 <- c(error= calc_class_err(y_test2, knn_model_2, N_test2))
    # err_k <- c(err_k, calc_class_err(y_test2, knn_model_predictions_2, n_test))
    
    if (isTRUE(all.equal(dim(confusion_matrix2), c(2, 2)))) {
      output2 <- c(output2, correct_negatives = confusion_matrix2[1,1],
                 correct_positives = confusion_matrix2[2,2], 
                 wrong_negatives = confusion_matrix2[2,1], 
                 wrong_positives = confusion_matrix2[1,2])
    }
    output2
  } 
  colMeans(out2)
}
rownames(output_values_2) <- paste("K", k_to_try, sep = "")

error2 <- output_values_2[ ,"error"]
confusion_out_2 <- output_values_2[,2:5]
best_rmse_k2 = k_to_try[which.min(error2)]
best_rmse_k2

rmse_best_k2 = min(error2)
rmse_best_k2

```


The following chart shows the relationship between the value of K and classification error. This chart helps us to find the optimal K with the lowest classification error.

```{r  }
plot(
  k_to_try,
  error2,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "k, number of neighbors",
  ylab = "classification error",
  main = "(Test) Error Rate vs Neighbors for KNN Classification"
)
```

```{r  }
min(unlist(output2))
which(output2 == min(unlist(output2)))

# table(y_test2) %>% kable() %>% kable_styling()
```

There is an average of 49% of Viral and 51% Nonviral in the test set.
```{r  }
mean(y_test2 == "1")
mean(y_test2 == "0")
```

Below, the confusion matrix for the KNN classification method is displayed.
```{r  }
confusion_out_2 = table(knn.pred = knn_model_2, Actual = articles[-train_ind2,]$viral) 

# 2067 number of Viral in real which is Viral in predicted. 
# 2336 number of Nonviral in real which is Nonviral in predicted. 
# 1654 number of Nonviral in real which is Viral in predicted. 
# 1872 number of Viral in tral which is Nonviral in predicted. 
colnames(confusion_out_2) <- c("Not Viral","Viral")
rownames(confusion_out_2) <- c("Not Viral","Viral")
confusion_out_2
```

The out-of-sample accuracy is shown below.
```{r  }
# 56% of the observations are correctly predicted by using this model. 
sum(diag(confusion_out_2))/sum(confusion_out_2) # out-of-sample accuracy
```

The overall error rate is shown below.
```{r  }
# Overall error rate is 44%
1 - sum(diag(confusion_out_2))/sum(confusion_out_2)
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r  }
# The true positive rate is 54%, which means 54% percentage of Viral that are correctly identified.
TP = confusion_out_2[2,2] / sum(confusion_out_2[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r  }
# The true negative rate is 56%, which means 56% percentage of Nonviral that are correctly identified.
TN = confusion_out_2[1,1] / sum(confusion_out_2[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r  }
# There are 43% of Viral incorrectly identified as Nonviral 
FP = confusion_out_2[2,1] / sum(confusion_out_2[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r  }
# There are 45% of NonViral incorrectly identified as Viral 
FN = confusion_out_2[1,2] / sum(confusion_out_2[,2])
FN

```

## Comparing Approach Performance
Averaged across 30 train-test splits, the KNN regression approach (regress first and threshold second) had an out-of-sample accuracy rate of around 0.546, whereas the KNN classification approach (threshold first and regress/classify second) had about 0.617 accuracy out-of-sample.

The KNN regression method has a smaller optimal K value where the error is lowest (K = 4), as opposed to the KNN classification method which has an optimal K that is much higher.

As for why the classification technique outperforms the model where we regress then threshold, we must look at the difference between regression and classification. The regression model predicts a numerical value, shares, based on several predictors. This predicted value can be greater than the bounds of probability between 0 and 1. In the case where the threshold clearly delineates between viral and not viral, then the regress first, threshold second method would be fine. However, when there are outliers, such as a few articles that get several hundred thousand shares as in this dataset, then the regression model's predictions would be affected. The linear regression slope itself would lean towards the outliers, while the threshold stays at 1,400 shares. As a result, articles that were previously classified as viral are now predicted to not be viral. The classification method has higher accuracy because the nearest neighbors are either 1 or 0. The difference in magnitude is not accounted for, so any articles with many shares do not affect the estimate of viral or not viral.