library(mosaic)
library(tidyverse)
library(FNN)
library(class)
library(car)
library(foreach)
library(ggplot2)
library(caret)

arti = read.csv("~/Desktop/SDS 323/Exercises/Exercise 2/data/online_news.csv")
names(arti)
summary(arti)

# Creat a new variables 
arti <- arti %>% 
  mutate(viral = ifelse(shares > 1400, 1, 0))

# Test the multicollinearity
lm0 = lm(shares ~ . - url, data = arti)
summary(lm0)

lm1 = lm(shares ~ . - url - is_weekend                             - weekday_is_sunday, data = arti)

summary(lm1)
vif(lm1)
# Remove: self_reference_min_shares,  self_reference_max_shares, min_negative_polarity 



# Make a train - test split 
N = nrow(arti)
N_train = floor(0.8 * N)
N_test = N - N_train

train_ind = sample.int(N, N_train, replace=FALSE)
test_ind = setdiff(1:N, train_ind)



D_train = arti[train_ind,]
D_test = arti[-train_ind,]

X_train = model.matrix(~.-(url + is_weekend 
                           + weekday_is_sunday + viral + shares)- 1, data=D_train)

y_train = select(D_train, shares)

X_test = model.matrix(~.- (url + is_weekend 
                            + weekday_is_sunday + viral + shares) - 1, data=D_test)

y_test = select(D_test, shares)


# KNN 
knn3 = knn.reg(train = X_train, 
               test = X_test, y = y_train, k=3)

ypred_knn3 <- knn3$pred

# Compare the models by RMSE

rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}

# Calculate the root mean square error
rmse(y_test, ypred_knn3)

#Let's try many values of K:
N = nrow(arti)
N_train = floor(0.8 * N)
N_test = N - N_train


rmse_vals = do(10) * {
  
  
  train_ind = sample.int(N, N_train)
  test_ind = setdiff(1:N, train_ind)
  D_train = arti[train_ind,]
  D_test = arti[-train_ind,]
  

  knn3 = knn.reg(train = X_train, 
                 test = X_test, y = y_train, k=3)
  names(knn3)
  ypred_knn3 <- knn3$pred
  
  rmse(y_test, ypred_knn3)
  
}

rmse_vals
colMeans(rmse_vals)
boxplot(rmse_vals)


# Out of sample performance 
set.seed(1)
knn3 = knn.reg(train = X_train, 
               test = X_test, y = y_train, k=100)
ypred_knn3 <- knn3$pred


yhat_test_viral1 = ifelse(ypred_knn3 > 1400,1, 0)
confusion_out = table(knn.pred = yhat_test_viral1, Actural = D_test$viral) 
summary(confusion_out)
confusion_out
# Matrix
colnames(confusion_out) <- c("Not Viral","Viral")
rownames(confusion_out) <- c("Not Viral","Viral")
confusion_out

# Sensitivity (SN) is calculated as the number of correct positive predictions divided by the total number of positives.
# SN = TP / (TP + FN)
TPR = mean(confusion_out[2,2] / sum(confusion_out[,2]))
TP

# Specificity (SP) is calculated as the number of correct negative predictions divided by the total number of negatives. # SP = TNR = TN / (TN + FP)

TNR = mean(confusion_out[1,1] / sum(confusion_out[,1]))
TN
# Overall Error Rate ERR = (FN + FP) 
FN = confusion_out[1,2] / sum(confusion_out[,2])
FP = confusion_out[2,1] / sum(confusion_out[,1])



############################
# Second Pass
standardized.X = scale(arti[, -1])
# Check 
var(arti[,2])
var(standardized.X[,2])

y = arti$viral

# Make a train - test split 
N = nrow(arti)
N_train2 = floor(0.8 * N)
N_test2 = N - N_train2

train_ind2 = sample.int(N, N_train2, replace=FALSE)
D_train2 = arti[train_ind2,]
D_test2 = arti[-train_ind2,]

X_train2 = model.matrix(~.-(url + is_weekend 
                           + weekday_is_sunday + viral + shares) - 1, data=D_train2)

y_train2 = y[train_ind2]

X_test2 = model.matrix(~.- ( url + is_weekend 
                            + weekday_is_sunday + viral + shares) - 1, data=D_test2)

y_test2 = y[-train_ind2]


# KNN 
knn3_2 = knn(train = X_train2, 
               test = X_test2, cl = y_train2, k=3)


# put the data and predictions in a single data frame
knn_trainset = data.frame(X_train2, type = y_train2)
knn3_testset = data.frame(X_test2, type = y_test2, 
                          type_pred = knn3_2)

# Make a table of classification errors
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}

calc_class_err(actual = y_test2,
               predicted = knn(train = X_train2,
                               test  = X_test2,
                                  cl = y_train2,
                                  k = 100))

# Find a good K
k_to_try = 1:30
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred = knn(train = scale(X_train2), 
             test  = scale(X_test2), 
             cl    = y_train2, 
             k     = k_to_try[i])
  err_k[i] = calc_class_err(y_test2, pred)
}
  
plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")

min(err_k)
which(err_k == min(err_k))
table(y_test2)
mean(y_test2 == "1")
mean(y_test2 == "0")

# Out of sample performance 
set.seed(1)
confusion_out_2 = table(knn.pred = knn3_2, Actu = D_test2$viral) 

confusion_out_2
# Matrix
colnames(confusion_out_2) <- c("Not Viral","Viral")
rownames(confusion_out_2) <- c("Not Viral","Viral")
confusion_out_2

# Out of sample accuracy (True Positive)
TP = confusion_out_2[2,2] / sum(confusion_out_2[,2])
TP
TN = confusion_out_2[1,1] / sum(confusion_out_2[,1])
TN
# Overall Error Rate 
FN = confusion_out_2[1,2] / sum(confusion_out_2[,2])
FN
FP = confusion_out_2[2,1] / sum(confusion_out_2[,1])
FP



