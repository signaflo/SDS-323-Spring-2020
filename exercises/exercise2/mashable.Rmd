---
title: "SDS 323 Exercises 2"
author: "Kyle Carter, Jacob Rachiele, Crystal Tse, Jinfang Yan"
date: "3/13/2020"
output: HTML_document
# output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
library(mosaic)
library(tidyverse)
library(FNN)
library(class)
library(caret)
library(foreach)
library(ggplot2)

library(kableExtra)
library(knitr)
library(ggthemes)

set.seed(1001)
articles = read.csv("online_news.csv")
articles <- articles %>% mutate(viral = ifelse(shares > 1400, 1, 0))

```
# Problem 3
## Becoming "Viral"
Mashable currently has about 49% of its articles go "viral," or get shared more than 1,400 times. Is there any way to improve this, or is it completely random?

There are several factors that can be considered. These include, but are not limited to, the subject matter (e.g., "Entertainment", "World", "Tech"), day of publication, and polarity of the words in the article's content and title.
 
## KNN Regression Approach
We fit a linear regression model to predict the number of times an article was shared using several variables, including the number of words in the title, number of links (in general and to other Mashable articles), subject matter, and average negative polarity of the article. These variables were selected based upon their statistical significance in a regression with all of the variables in the dataset used to predict number of shares.

A comprehensive overview of the variables used in our regression model is summarized in the table below.

__Table 1__

| Variable Name                 | Description                                             |
|-------------------------------|---------------------------------------------------------|
| n_tokens_title                | Number of words in the title                            |
| num_hrefs                     | Number of links                                         |
| num_self_hrefs                | Number of links to other articles published by Mashable |
| num_imgs                      | Number of images                                        |
| average_token_length          | Average length of the words in the content              |
| num_keywords                  | Number of keywords in the metadata                      |
| data_channel_is_lifestyle     | Is data channel 'Lifestyle'?                            |
| data_channel_is_entertainment | Is data channel 'Entertainment'?                        |
| data_channel_is_bus           | Is data channel 'Business'?                             |
| data_channel_is_socmed        | Is data channel 'Social Media'?                         |
| data_channel_is_tech          | Is data channel 'Tech'?                                 |
| data_channel_is_world         | Is data channel 'World'?                                |
| self_reference_min_shares     | Min. shares of referenced articles in Mashable          |
| avg_negative_polarity         | Avg. polarity of negative words                         |


These variables that help in explaining the number of times an article was shared also have an intuitive reasoning behind their inclusion in the model. For example, the number of words in the title might contribute to how "click-worthy" a certain article is, increasing the likelihood that is shared more frequently. Links to an article allow for easy access to its contents, and more keywords in the metadata make an article more likely to found by individuals that then share it.
```{r echo = FALSE}

rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}

```
Below are the different values of K tested to find the optimal K for the out-of-sample accuracy.

```{r echo = FALSE}

# confusion matrix - make a table of KNN (regular, not classification) errors
k_grid <- exp(seq(log(1), log(100), length=33)) %>% round %>% unique
k_grid <- k_grid[k_grid != 2]
k_grid
```
<!-- Iterating across the above range of K to find the best out-of-sample accuracy rate, we find that the best K value is when K = 4. A potential reason for why averaging 4 points yields the best out-of-sample accuracy despite such a large dataset (almost 40,000 observations) is  _____________ -->

Iterating across the above range of K to find the best out-of-sample accuracy rate, we find that the best K value is when K = 4. This makes sense because for KNN regression, the best results occur with a very large value of K, corresponding to a small value of 1/K.
```{r echo = FALSE}
# create a confusion matrix for multiple K values
# Make a train - test split 
N = nrow(articles)
N_train = floor(0.8 * N)
N_test = N - N_train

rmse_grid = list()

repetitions <- 10
confusion_matrix_values = foreach(k = k_grid, .combine = rbind) %do% {
  out = do(repetitions) * {
    # re-split into train and test cases with the same sample sizes
    train_ind = sample.int(N, N_train, replace=FALSE)
    
    D_train = articles[train_ind,]
    D_test = articles[-train_ind,]
    
    
    X_train = model.matrix(~ n_tokens_title +
                             num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                             num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                             data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech +
                             data_channel_is_world + self_reference_min_shares  +
                             avg_negative_polarity - 1,  data = D_train)
    
    y_train = as.numeric(dplyr::select(D_train, shares)$shares)
    
    
    X_test = model.matrix(~ n_tokens_title +
                             num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                             num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                             data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech +
                             data_channel_is_world + self_reference_min_shares  +
                             avg_negative_polarity - 1, data = D_test)
    
    y_test = as.numeric(dplyr::select(D_test, shares)$shares)
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train = scale(X_train, scale=scale_factors)
    # scale the test set features using the same scale factors
    X_test = scale(X_test, scale=scale_factors)
   
    # KNN 
    knn_model = knn.reg(train = X_train, 
                   test = X_test, y = y_train, k=k)
    
    knn_model_predictions = knn_model$pred
    
    # out of sample confusion matrix
    viral_prediction = ifelse(knn_model_predictions > 1400, 1, 0)
    confusion_matrix = table(y = D_test$viral, yhat = viral_prediction)
    if (isTRUE(all.equal(dim(confusion_matrix), c(2, 2)))) {
      c(correct_negatives = confusion_matrix[1,1], correct_positives = confusion_matrix[2,2],
        wrong_negatives = confusion_matrix[2,1], wrong_positives = confusion_matrix[1,2])
    }
    
    rmse_grid <- c(rmse_grid, rmse(y_test, knn_model$pred))
  }
  colMeans(out)
}
rownames(confusion_matrix_values) <- paste("K", k_grid, sep = "")
accuracy <- rowSums(confusion_matrix_values[,1:2]) / rowSums(confusion_matrix_values)
best_acc_k = k_grid[which.max(accuracy)]
best_acc_k

plot(
  k_grid,
  accuracy,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "Accuracy Rate",
  main = "KNN Regression Accuracy"
)

```

Below is a plot to find the optimal K based on minimizing the RMSE. From this, we find that the best K value is when K = 87.
```{r echo = FALSE}
best_k = k_grid[which(rmse_grid == min(unlist(list(rmse_grid))))]
best_k


plot(
  k_grid,
  rmse_grid,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "RMSE",
  main = "KNN Regression RMSE"
)

```



```{r echo = FALSE}
knn_model = knn.reg(train = X_train, test = X_test, y = y_train, k=best_k)
knn_model_predictions <- knn_model$pred


yhat_test_viral1 = ifelse(knn_model_predictions > 1400,1, 0)
```

The confusion matrix is shown below.
```{r echo = FALSE}
confusion_out = table(knn.pred = yhat_test_viral1, Actual = D_test$viral) 

# Matrix
colnames(confusion_out) <- c("Not Viral","Viral")
rownames(confusion_out) <- c("Not Viral","Viral")
confusion_out %>% kable() %>% kable_styling()
```

The out-of-sample accuracy is shown below.
```{r echo = FALSE}
sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy
```

The overall error rate is shown below.
```{r echo = FALSE}
# overall error rate
1 - sum(diag(confusion_out))/sum(confusion_out)
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r echo = FALSE}
TP = confusion_out[2,2] / sum(confusion_out[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r echo = FALSE}
# The true negative rate is --%, which means --% percentage of Nonviral that are correctly identified.
TN = confusion_out[1,1] / sum(confusion_out[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r echo = FALSE}
# There are --% of Viral incorrectly identified as Nonviral 
FP = confusion_out[2,1] / sum(confusion_out[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r echo = FALSE}
# There are --% of NonViral incorrectly identified as Viral 
FN = confusion_out[1,2] / sum(confusion_out[,2])
FN
```

In contrast, the null model, which always predicts that an article will not be viral, has an out-of-sample accuracy rate of 0.5065584. The above KNN regression model has a slightly higher out-of-sample accuracy rate of 0.5524026; however, it is inherently difficult to predict whether an article will go viral, so this number is still relatively low.

```{r echo = FALSE}
# null model, always predicts "not viral"
lm_lame = lm(!viral ~ 1, data = articles)

# do the TP for null
coef(lm_lame)
# do false negative
## debug
# nonviral in real?
```
## KNN Classification Approach

Approaching the problem from where the dependent variable is a binary, or dummy, variable, rather than a numerical variable, yields different results. We defined a dummy variable "Viral" that equals 1 when the number of shares is greater than 1400. 

```{r echo = FALSE}
############################ 

standardized.X = scale(articles[, -1])
var(articles[,2])
var(standardized.X[,2])
y = articles$viral

N = nrow(articles)
N_train2 = floor(0.8 * N)
N_test2 = N - N_train2
train_ind2 = sample.int(N, N_train2, replace=FALSE)
D_train2 = articles[train_ind2,]
D_test2 = articles[-train_ind2,]
X_train2 = model.matrix(~ 
                          n_tokens_title + 
                          num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                          num_keywords + data_channel_is_lifestyle +
                          data_channel_is_entertainment +
                          data_channel_is_bus + data_channel_is_socmed +
                          data_channel_is_tech + data_channel_is_world +
                          self_reference_min_shares  + avg_negative_polarity - 1, 
                        data=D_train2)
y_train2 = y[train_ind2]
X_test2 = model.matrix(~
                         n_tokens_title +
                         num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                         num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                         data_channel_is_bus + data_channel_is_socmed +
                         data_channel_is_tech + data_channel_is_world +
                         self_reference_min_shares  + avg_negative_polarity - 1,
                       data = D_test2
)
y_test2 = y[-train_ind2]

knn3_2 = knn(train = X_train2, 
             test = X_test2, cl = y_train2, k=3)

knn_trainset = data.frame(X_train2, type = y_train2)
knn3_testset = data.frame(X_test2, type = y_test2, 
                          type_pred = knn3_2)

calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}

# calc_class_err(actual = y_test2,
#                predicted = knn(train = X_train2,
#                                test  = X_test2,
#                                cl = y_train2,
#                                k = 100))

# k_to_try = 1:30
# k_to_try = 1:50
k_to_try = k_grid
err_k = rep(x = 0, times = length(k_to_try))
for (i in seq_along(k_to_try)) {
  pred = knn(train = scale(X_train2),
             test  = scale(X_test2),
             cl    = y_train2,
             k     = k_to_try[i])
  err_k[i] = calc_class_err(y_test2, pred)
}
# k_grid = k_grid[0:10]
# for (i in seq_along(k_grid)) {
#   pred = knn(train = scale(X_train2),
#              test  = scale(X_test2),
#              cl    = y_train2,
#              k     = k_grid[i])
#   err_k[i] = calc_class_err(y_test2, pred)
# }

```

The following chart shows the relationship between the value of K and classification error. This chart helps us to find the optimal k = 28 has the smallest classification error of 38%.

## debug get rid of hard coded numbers
```{r echo = FALSE}
plot(
  err_k,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "k, number of neighbors",
  ylab = "classification error",
  main = "(Test) Error Rate vs Neighbors for KNN Classification"
)
```

```{r echo = FALSE}
min(err_k)
cat('Min Error Rate with K', which(err_k == min(err_k)))
table(y_test2)
```

There is an average of 49% of Viral and 51% Nonviral in the test set.
```{r echo = FALSE}
mean(y_test2 == "1")
mean(y_test2 == "0")
```

Below, the confusion matrix for the KNN classification method is displayed.
```{r echo = FALSE}
confusion_out_2 = table(knn.pred = knn3_2, Actual = D_test2$viral) 

# 2067 number of Viral in real which is Viral in predicted. 
# 2336 number of Nonviral in real which is Nonviral in predicted. 
# 1654 number of Nonviral in real which is Viral in predicted. 
# 1872 number of Viral in tral which is Nonviral in predicted. 
colnames(confusion_out_2) <- c("Not Viral","Viral")
rownames(confusion_out_2) <- c("Not Viral","Viral")
confusion_out_2
```

The out-of-sample accuracy is shown below.
```{r echo = FALSE}
# 56% of the observations are correctly predicted by using this model. 
sum(diag(confusion_out_2))/sum(confusion_out_2) # out-of-sample accuracy
```

The overall error rate is shown below.
```{r echo = FALSE}
# Overall error rate is 44%
1 - sum(diag(confusion_out_2))/sum(confusion_out_2)
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r echo = FALSE}
# The true positive rate is 54%, which means 54% percentage of Viral that are correctly identified.
TP = confusion_out_2[2,2] / sum(confusion_out_2[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r echo = FALSE}
# The true negative rate is 56%, which means 56% percentage of Nonviral that are correctly identified.
TN = confusion_out_2[1,1] / sum(confusion_out_2[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r echo = FALSE}
# There are 43% of Viral incorrectly identified as Nonviral 
FP = confusion_out_2[2,1] / sum(confusion_out_2[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r echo = FALSE}
# There are 45% of NonViral incorrectly identified as Viral 
FN = confusion_out_2[1,2] / sum(confusion_out_2[,2])
FN

```

## Comparing Approach Performance
Averaged across ___ train-test splits, the KNN regression approach (regress first and threshold second) had an out-of-sample accuracy rate of around 0.552, whereas the KNN classification approach (threshold first and regress/classify second) had about 0.556 accuracy out-of-sample.

The KNN regression method has a smaller optimal K value where the error is lowest (K = 4), as opposed to the KNN classification method which has an optimal K that is much higher.

