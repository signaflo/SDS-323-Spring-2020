---
title: "SDS 323 Exercises 2"
author: "Kyle Carter, Jacob Rachiele, Crystal Tse, Jinfang Yan"
date: "3/13/2020"
output: HTML_document
# output: md_document
---

```{r setup, include = FALSE, message = FALSE, echo = FALSE}
library(mosaic)
library(tidyverse)
library(FNN)
library(class)
library(caret)
library(foreach)
library(ggplot2)

library(kableExtra)
library(knitr)
library(ggthemes)

arti = read.csv("online_news.csv")

```
# Becoming "Viral"
Mashable currently has about 49% of its articles go "viral," or get shared more than 1,400 times. Is there any way to improve this, or is it completely random?

There are several factors that can be considered. These include, but are not limited to, the subject matter (e.g., "Entertainment", "World", "Tech"), day of publication, and polarity of the words in the article's content and title.
 
A comprehensive overview of mean numeric data used in this analysis is summarized in the table below. The variable "url" was omitted for concision.

```{r echo = FALSE}
# head(arti) %>% kable() %>% kable_styling()
# summarize_all(arti, list(~mean(.)))
arti %>% summarize_if(is.numeric, mean)
```

```{r echo = FALSE}
arti <- arti %>% 
  mutate(viral = ifelse(shares > 1400, 1, 0))


# Make a train - test split 
N = nrow(arti)
N_train = floor(0.8 * N)
N_test = N - N_train

train_ind = sample.int(N, N_train, replace=FALSE)

D_train = arti[train_ind,]
D_test = arti[-train_ind,]

D_test = arrange(D_test, n_tokens_content)
# head(D_test)

X_train = model.matrix( ~ n_tokens_title + 
                         num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                         num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                         data_channel_is_bus + data_channel_is_socmed +
                         data_channel_is_tech + data_channel_is_world +
                         self_reference_min_shares  + avg_negative_polarity - 1, data=D_train)

y_train = dplyr::select(D_train, shares)

X_test = model.matrix( ~ n_tokens_title + 
                        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                        num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                        data_channel_is_bus + data_channel_is_socmed +
                        data_channel_is_tech + data_channel_is_world +
                        self_reference_min_shares  + avg_negative_polarity - 1, data=D_test)

y_test = dplyr::select(D_test, shares)

# # scale the training set features
# scale_factors = apply(X_train, 2, sd)
# X_train = scale(X_train, scale=scale_factors)
# 
# # scale the test set features using the same scale factors
# X_test = scale(X_test, scale=scale_factors)


# KNN 
knn3 = knn.reg(train = X_train, 
               test = X_test, y = y_train, k=3)
# names(knn3)

#####
# Compare the models by RMSE_out
#####
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}


ypred_knn3 = knn3$pred

# Calculate the root mean square error 
# Compare predictions with test articles
# rmse(y_test, ypred_knn3)


#Plot the fit 

D_test$ypred_knn3 = ypred_knn3

```
Below are the different values of K tested to find the optimal K for the out-of-sample accuracy.

```{r echo = FALSE}

# confusion matrix - make a table of KNN (regular, not classification) errors
# first use the binary responses to get a confusion matrix of probabilities
k_grid <- exp(seq(log(1), log(10000), length=30)) %>% round %>% unique
k_grid <- k_grid[k_grid != 2]
k_grid
```
Iterating across the above range of K to find the best out-of-sample accuracy rate, we find that the best K value is when K = 4. A potential reason for why averaging 4 points yields the best out-of-sample accuracy despite such a large dataset (almost 40,000 observations) is  
```{r echo = FALSE}
# create a confusion matrix for multiple K values
set.seed(1)
confusion_valse = 
  foreach(k = k_grid,  .combine='c') %do% {
  out = do(10)*{
    # re-split into train and test cases with the same sample sizes
    train_ind = sample.int(N, N_train, replace=FALSE)
    
    D_train = arti[train_ind,]
    D_test = arti[-train_ind,]
    
    D_test = arrange(D_test, n_tokens_content)
    
    X_train = model.matrix(~ 
                             n_tokens_title + 
                             num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                             num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
                             data_channel_is_bus + data_channel_is_socmed +
                             data_channel_is_tech + data_channel_is_world +
                             self_reference_min_shares  + avg_negative_polarity - 1, data=D_train)
    
    y_train = dplyr::select(D_train, shares)
    
    
    X_test = model.matrix(
      ~  n_tokens_title +
        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
        num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
        data_channel_is_bus + data_channel_is_socmed +
        data_channel_is_tech + data_channel_is_world +
        self_reference_min_shares  + avg_negative_polarity - 1,
      data = D_test
    )
    
    y_test = dplyr::select(D_test, shares)
    
    # scale the training set features
    # scale_factors = apply(X_train, 2, sd)
    # X_train = scale(X_train, scale=scale_factors)
    # 
    # # scale the test set features using the same scale factors
    # X_test = scale(X_test, scale=scale_factors)
    
    # KNN 
    knn3 = knn.reg(train = X_train, 
                   test = X_test, y = y_train, k=k)
    
    ypred_knn3 = knn3$pred
    # out of sample confusion matrix
    viral_prediction = ifelse(ypred_knn3 > 1400, 1, 0)
    confusion_out = table(y = D_test$viral, yhat = viral_prediction)
    # confusion_out
    
    sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy
    
    # # overall error rate
    # 1 - sum(diag(confusion_out))/sum(confusion_out)
    # # true positive rate
    # confusion_out[2,2]/(confusion_out[2, 1] + confusion_out[2,2])
    # # false positive rate 
    # confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
    # #across multiple train/test splits!!!!!
  }
  mean(out$result)
}
confusion_valse
mean(confusion_valse[[1]])

max(confusion_valse)
# best_k = which(err_k == max(confusion_valse))
best_k = k_grid[which(confusion_valse == max(confusion_valse))]
best_k

plot(k_grid, confusion_valse)

# got 0.500454  (do 20x), as opposed to guessing "not viral" which got 0.5065584
# 0.5056123 with K = 1 through 300 by 2
# aim for 0.53 (another group at office hours)
# 0.54732 average with k_grid <- exp(seq(log(1), log(500), length=35)) %>% round %>% unique
#   and out = do * 10

#0.5482785 with k_grid <- exp(seq(log(1), log(900), length=30)) %>% round %>% unique
```


```{r echo = FALSE}
# Aubrey's version of above^^
# Out of sample performance 
set.seed(1)
knn3 = knn.reg(train = X_train, 
               test = X_test, y = y_train, k=best_k)  
ypred_knn3 <- knn3$pred


yhat_test_viral1 = ifelse(ypred_knn3 > 1400,1, 0)
confusion_out = table(knn.pred = yhat_test_viral1, Actual = D_test$viral) 
# summary(confusion_out)
# confusion_out

# Matrix
colnames(confusion_out) <- c("Not Viral","Viral")
rownames(confusion_out) <- c("Not Viral","Viral")
confusion_out %>% kable() %>% kable_styling()

sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy

# overall error rate
1 - sum(diag(confusion_out))/sum(confusion_out)
# true positive rate
TP = confusion_out[2,2]/(confusion_out[2, 1] + confusion_out[2,2])
TP
# false positive rate
FP = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
FP

# null model, always predicts "not viral"
lm_lame = lm(!viral ~ 1, data = arti)

# do the TP for null
coef(lm_lame)
# do false negative
## debug
# nonviral in real?
```

Approaching the problem from where the dependent variable is a binary, or dummy, variable, rather than a numerical variable, yields different results. More specifically, _______________________________________________.

```{r echo = FALSE}
############################
# Second Pass: KNN Classification
standardized.X = scale(arti[, -1])
# Check 
var(arti[,2])
var(standardized.X[,2])

y = arti$viral


# Make a train - test split 
N = nrow(arti)
N_train2 = floor(0.8 * N)
N_test2 = N - N_train2

train_ind2 = sample.int(N, N_train2, replace=FALSE)
D_train2 = arti[train_ind2,]
D_test2 = arti[-train_ind2,]

X_train2 = model.matrix(~ 
                             n_tokens_title + 
                             num_hrefs + num_self_hrefs + num_imgs + average_token_length +
                             num_keywords + data_channel_is_lifestyle +
                             data_channel_is_entertainment +
                             data_channel_is_bus + data_channel_is_socmed +
                             data_channel_is_tech + data_channel_is_world +
                             self_reference_min_shares  + avg_negative_polarity - 1, 
                             data=D_train2)

y_train2 = y[train_ind2]

X_test2 = model.matrix(~
    n_tokens_title +
    num_hrefs + num_self_hrefs + num_imgs + average_token_length +
    num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
    data_channel_is_bus + data_channel_is_socmed +
    data_channel_is_tech + data_channel_is_world +
    self_reference_min_shares  + avg_negative_polarity - 1,
  data = D_test2
)

y_test2 = y[-train_ind2]


# KNN 
knn3_2 = knn(train = X_train2, 
             test = X_test2, cl = y_train2, k=3)


# put the data and predictions in a single data frame
knn_trainset = data.frame(X_train2, type = y_train2)
knn3_testset = data.frame(X_test2, type = y_test2, 
                          type_pred = knn3_2)


# Make a table of classification errors
calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}

calc_class_err(actual = y_test2,
               predicted = knn(train = X_train2,
                               test  = X_test2,
                               cl = y_train2,
                               k = 100))


# Find a good K
## debug do this across a much larger range for K
k_to_try = 1:30
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred = knn(train = scale(X_train2), 
             test  = scale(X_test2), 
             cl    = y_train2, 
             k     = k_to_try[i])
  err_k[i] = calc_class_err(y_test2, pred)
}

plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")

min(err_k)
which(err_k == min(err_k))
table(y_test2)
mean(y_test2 == "1")
mean(y_test2 == "0")

# Out of sample performance 
set.seed(1)
confusion_out_2 = table(knn.pred = knn3_2, Actu = D_test2$viral) 

confusion_out_2
# Matrix
colnames(confusion_out_2) <- c("Not Viral","Viral")
rownames(confusion_out_2) <- c("Not Viral","Viral")
confusion_out_2

# Out of sample accuracy
sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy

# overall error rate
1 - sum(diag(confusion_out))/sum(confusion_out)
# true positive rate
TP = confusion_out[2,2]/(confusion_out[2, 1] + confusion_out[2,2])
TP
# false positive rate
FP = confusion_out[1,2]/(confusion_out[1,1] + confusion_out[1,2])
FP
```

