---
title: "mashable"
author: "Kyle Carter, Jacob Rachiele, Crystal Tse, Jinfang Yan"
date: "3/13/2020"
# output: HTML_document
output: md_document
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(  message = FALSE, warning = FALSE)

```

```{r}
library(mosaic)
library(tidyverse)
library(FNN)
library(class)
library(caret)
library(foreach)
library(ggplot2)

library(kableExtra)
library(knitr)
library(ggthemes)


library(mosaic)
# library(doMC)

set.seed(1001)
articles = read.csv("online_news.csv")
articles <- articles %>% mutate(viral = ifelse(shares > 1400, 1, 0))
```
# Problem 3
## Becoming "Viral"
Mashable currently has about 49% of its articles go "viral," or get shared more than 1,400 times. Is there any way to improve this, or is it completely random?

There are several factors that can be considered. These include, but are not limited to, the subject matter (e.g., "Entertainment", "World", "Tech"), day of publication, and polarity of the words in the article's content and title.
 
## KNN Regression Approach
We fit a linear regression model to predict the number of times an article was shared using several variables, including the number of words in the title, number of links (in general and to other Mashable articles), subject matter, and average negative polarity of the article. These variables were selected based upon their statistical significance in a regression with all of the variables in the dataset used to predict number of shares.

A comprehensive overview of the variables used in our regression model is summarized in the table below.

__Table 1__

| Variable Name                 | Description                                             |
|-------------------------------|---------------------------------------------------------|
| n_tokens_title                | Number of words in the title                            |
| num_hrefs                     | Number of links                                         |
| num_self_hrefs                | Number of links to other articles published by Mashable |
| num_imgs                      | Number of images                                        |
| average_token_length          | Average length of the words in the content              |
| num_keywords                  | Number of keywords in the metadata                      |
| data_channel_is_lifestyle     | Is data channel 'Lifestyle'?                            |
| data_channel_is_entertainment | Is data channel 'Entertainment'?                        |
| data_channel_is_bus           | Is data channel 'Business'?                             |
| data_channel_is_socmed        | Is data channel 'Social Media'?                         |
| data_channel_is_tech          | Is data channel 'Tech'?                                 |
| data_channel_is_world         | Is data channel 'World'?                                |
| self_reference_min_shares     | Min. shares of referenced articles in Mashable          |
| avg_negative_polarity         | Avg. polarity of negative words                         |


These variables that help in explaining the number of times an article was shared also have an intuitive reasoning behind their inclusion in the model. For example, the number of words in the title might contribute to how "click-worthy" a certain article is, increasing the likelihood that is shared more frequently. Links to an article allow for easy access to its contents, and more keywords in the metadata make an article more likely to found by individuals that then share it.
```{r}
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```
Below are the different values of K tested to find the optimal K for the out-of-sample accuracy.

```{r}

# confusion matrix - make a table of KNN (regular, not classification) errors
k_grid <- exp(seq(log(1), log(50), length=10)) %>% round %>% unique
k_grid <- k_grid[k_grid != 2]
k_grid

```

Iterating across the above range of K to find the lowest RMSE (Root Mean Square Error), we try to find the best K.
```{r}

X = select(articles, n_tokens_title, 
               num_hrefs, num_self_hrefs, num_imgs, average_token_length,
               num_keywords, data_channel_is_lifestyle, data_channel_is_entertainment,
               data_channel_is_bus, data_channel_is_socmed,
               data_channel_is_tech, data_channel_is_world,
               self_reference_min_shares , avg_negative_polarity)
y = articles$shares
n = length(y)
n_train = round(0.8*n)
n_test = n - n_train

# create a confusion matrix for multiple K values
# Make a train - test split 
repetitions <- 30
output_values = foreach(k = k_grid, .combine='rbind') %do% {
  out = do(repetitions)*{
    train_ind = sample.int(n, n_train)
    X_train = X[train_ind,]
    X_test = X[-train_ind,]
    y_train = y[train_ind]
    y_test = y[-train_ind]
    
    X_train <- model.matrix(~ n_tokens_title +
               num_hrefs + num_self_hrefs + num_imgs + average_token_length +
               num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
               data_channel_is_bus + data_channel_is_socmed +
               data_channel_is_tech + data_channel_is_world +
               self_reference_min_shares  + avg_negative_polarity - 1, data = X_train)
    X_test <-  model.matrix(~ n_tokens_title + 
               num_hrefs + num_self_hrefs + num_imgs + average_token_length +
               num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
               data_channel_is_bus + data_channel_is_socmed +
               data_channel_is_tech + data_channel_is_world +
               self_reference_min_shares  + avg_negative_polarity - 1, data = X_test)
    
    # scale the training set features
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    knn_model = FNN::knn.reg(X_train_sc, X_test_sc, y_train, k=k)
    knn_model_predictions = knn_model$pred
    
    # out of sample confusion matrix
    viral_prediction = ifelse(knn_model_predictions > 1400, 1, 0)
    actual_viral <- ifelse(y_test > 1400, 1, 0)
    
    confusion_matrix = table(y = actual_viral, yhat = viral_prediction)
    output <- c(rmse = rmse(y_test, knn_model$pred))
    if (isTRUE(all.equal(dim(confusion_matrix), c(2, 2)))) {
      output <- c(output, correct_negatives = confusion_matrix[1,1],
                 correct_positives = confusion_matrix[2,2], 
                 wrong_negatives = confusion_matrix[2,1], 
                 wrong_positives = confusion_matrix[1,2])
    }
    output
  } 
  colMeans(out)
}
rownames(output_values) <- paste("K", k_grid, sep = "")

rmse_avg <- output_values[,"rmse"]
confusion_matrix_values <- output_values[,2:5]
best_rmse_k = k_grid[which.min(rmse_avg)]
best_rmse_k

rmse_best_k = min(rmse_avg)
rmse_best_k
# rmse_grid = data.frame(K = k_grid, RMSE = rmse_grid)
# ind_best = which.min(rmse_grid$RMSE)
# best_k = k_grid[ind_best]
# best_k
```
We find that the lowest RMSE is when K = `r best_rmse_k`. However, the K value with the highest out-of-sample accuracy has yet to be calculated.

```{r  }
accuracy <- rowSums(confusion_matrix_values[,1:2]) / rowSums(confusion_matrix_values)
best_acc_k = k_grid[which.max(accuracy)]
best_acc_k

rmse_most_accurate_k = rmse_avg[which(k_grid == best_acc_k)]
rmse_most_accurate_k

```
Above is the best K value for out-of-sample accuracy. Although it seems like a low K value is not optimal for minimizing RMSE since it would lead to overfitting of the data, K = `r best_acc_k` has a RMSE of `r rmse_most_accurate_k`, as opposed to K = `r best_rmse_k` with a RMSE of `r rmse_best_k`.

Below is a plot of the K against the out-of-sample accuracy rate. While the RMSE is lower for K = `r best_rmse_k`, creating a model that is applicable to new data and accurate is also a high priority.
```{r  }
plot(
  k_grid,
  accuracy,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "Accuracy Rate",
  main = "KNN Regression Accuracy",
  log = 'x'
)
```

It is best to have both a low RMSE and high accuracy rate. 

K = `r best_rmse_k` has an average accuracy rate of `r rmse_best_k`, error rate of 0.5065584, true positive rate of 0.9907716, and false positive rate of 0.9764151. 

Meanwhile, K = `r best_acc_k` has accuracy rate of 0.5541682, error rate of 0.4458318, true positive rate of 0.7754422, and false positive rate of 0.6601291.

Thus, K =  satisfies the criteria better than K = `r best_rmse_k`, so we find the optimal K based on a higher accuracy rate out-of-sample.
```{r  }


plot(
  k_grid,
  rmse_avg,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "K",
  ylab = "RMSE",
  main = "KNN Regression RMSE",
  log = 'x'
)

```



```{r  }
knn_model = knn.reg(train = X_train, test = X_test, y = y_train, k=best_acc_k)
knn_model_predictions <- knn_model$pred


yhat_test_viral1 = ifelse(knn_model_predictions > 1400,1, 0)
```

The confusion matrix for K = `r best_acc_k` is shown below.
```{r  }
confusion_out = table(knn.pred = yhat_test_viral1, Actual = articles[-train_ind, ]$viral) 

# Matrix
colnames(confusion_out) <- c("Not Viral","Viral")
rownames(confusion_out) <- c("Not Viral","Viral")
confusion_out %>% kable() %>% kable_styling()
```

The out-of-sample accuracy is shown below.
```{r  }
sum(diag(confusion_out))/sum(confusion_out) # out-of-sample accuracy
```

The overall error rate is shown below.
```{r  }
# overall error rate
1 - sum(diag(confusion_out))/sum(confusion_out)
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r  }
TP = confusion_out[2,2] / sum(confusion_out[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r  }
# The true negative rate is --%, which means --% percentage of Nonviral that are correctly identified.
TN = confusion_out[1,1] / sum(confusion_out[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r  }
# There are --% of Viral incorrectly identified as Nonviral 
FP = confusion_out[2,1] / sum(confusion_out[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r  }
# There are --% of NonViral incorrectly identified as Viral 
FN = confusion_out[1,2] / sum(confusion_out[,2])
FN
```

In contrast, the null model, which always predicts that an article will not be viral, has a true positive rate of 0.4934416. The above KNN regression model has a slightly higher out-of-sample accuracy rate of about 0.552; however, it is inherently difficult to predict whether an article will go viral, so this number is still relatively low.

```{r  }
# null model, always predicts "not viral"
lm_lame = lm(!viral ~ 1, data = articles)

# do the TP for null
1 - coef(lm_lame)
```
## KNN Classification Approach

Approaching the problem considering the dependent variable as a binary variable, rather than a numerical variable, yields different results. We defined a dummy variable "Viral" that equals 1 when the number of shares is greater than 1400. 


```{r  }

# calc_class_err = function(actual, predicted) {
#   mean(actual != predicted)
# }

calc_class_err = function(actual, predicted, n_testing) {
  mean((sum(actual != predicted))/n_testing)
}

y = articles$viral
N = nrow(articles)
N_train2 = floor(0.8 * N)
N_test2 = N - N_train2
    
    
# k_to_try = k_grid
k_to_try = list(3,4,7)
reps = 2

output_values_2 = foreach(k = k_to_try, .combine = rbind) %do% {
  out2 = do(reps) * {
    train_ind2 = sample.int(N, N_train2, replace = FALSE)
    # D_train2 = articles[train_ind2, ]
    # D_test2 = articles[-train_ind2, ]
    
    X_train2 = X[train_ind2,]
    X_test2 = X[-train_ind2,]
    y_train2 = y[train_ind2]
    y_test2 = y[-train_ind2]
    
    X_train2 = model.matrix(
      ~
        n_tokens_title +
        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
        num_keywords + data_channel_is_lifestyle +
        data_channel_is_entertainment +
        data_channel_is_bus + data_channel_is_socmed +
        data_channel_is_tech + data_channel_is_world +
        self_reference_min_shares  + avg_negative_polarity - 1,
      data = X_train2
    )
    
    X_test2 = model.matrix(
      ~
        n_tokens_title +
        num_hrefs + num_self_hrefs + num_imgs + average_token_length +
        num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment +
        data_channel_is_bus + data_channel_is_socmed +
        data_channel_is_tech + data_channel_is_world +
        self_reference_min_shares  + avg_negative_polarity - 1,
      data = X_test2
    )
    
    
    # scale the training set features
    scale_factors = apply(X_train2, 2, sd)
    X_train_sc2 = scale(X_train2, scale=scale_factors)
    
    # scale the test set features using the same scale factors
    X_test_sc2 = scale(X_test2, scale=scale_factors)
   
    knn_model_2 = class::knn(train = X_train_sc2,
               test  = X_test_sc2,
               cl    = y_train2,
               k     = k)
    # knn_model_predictions_2 = knn_model_2$pred
    
     # out of sample confusion matrix
    viral_prediction_2 = ifelse(knn_model_2 == 1, 1, 0)
    actual_viral <- ifelse(y_test2 == 1, 1, 0)
    
    confusion_matrix2 = table(y = actual_viral, yhat = viral_prediction_2)
    output2 <- c(error= calc_class_err(y_test2, knn_model_2, N_test2))
    # err_k <- c(err_k, calc_class_err(y_test2, knn_model_predictions_2, n_test))
    
    if (isTRUE(all.equal(dim(confusion_matrix2), c(2, 2)))) {
      output2 <- c(output2, correct_negatives = confusion_matrix2[1,1],
                 correct_positives = confusion_matrix2[2,2], 
                 wrong_negatives = confusion_matrix2[2,1], 
                 wrong_positives = confusion_matrix2[1,2])
    }
    output2
  } 
  colMeans(out2)
}
rownames(output_values_2) <- paste("K", k_to_try, sep = "")

error2 <- output_values_2[ ,"error"]
confusion_out_2 <- output_values_2[,2:5]
best_rmse_k2 = k_to_try[which.min(error2)]
best_rmse_k2

rmse_best_k2 = min(error2)
rmse_best_k2

```


The following chart shows the relationship between the value of K and classification error. This chart helps us to find the optimal K with the lowest classification error.

```{r  }
plot(
  k_to_try,
  output2,
  type = "b",
  col = "dodgerblue",
  cex = 1,
  pch = 20,
  xlab = "k, number of neighbors",
  ylab = "classification error",
  main = "(Test) Error Rate vs Neighbors for KNN Classification"
)
```

```{r  }
min(unlist(output2))
which(output2 == min(unlist(output2)))

# table(y_test2) %>% kable() %>% kable_styling()
```

There is an average of 49% of Viral and 51% Nonviral in the test set.
```{r  }
mean(y_test2 == "1")
mean(y_test2 == "0")
```

Below, the confusion matrix for the KNN classification method is displayed.
```{r  }
confusion_out_2 = table(knn.pred = knn_model_2, Actual = X_test2$viral) 

# 2067 number of Viral in real which is Viral in predicted. 
# 2336 number of Nonviral in real which is Nonviral in predicted. 
# 1654 number of Nonviral in real which is Viral in predicted. 
# 1872 number of Viral in tral which is Nonviral in predicted. 
colnames(confusion_out_2) <- c("Not Viral","Viral")
rownames(confusion_out_2) <- c("Not Viral","Viral")
confusion_out_2
```

The out-of-sample accuracy is shown below.
```{r  }
# 56% of the observations are correctly predicted by using this model. 
sum(diag(confusion_out_2))/sum(confusion_out_2) # out-of-sample accuracy
```

The overall error rate is shown below.
```{r  }
# Overall error rate is 44%
1 - sum(diag(confusion_out_2))/sum(confusion_out_2)
```

The true positive rate (percentage of Viral that are correctly identified) is shown below.
```{r  }
# The true positive rate is 54%, which means 54% percentage of Viral that are correctly identified.
TP = confusion_out_2[2,2] / sum(confusion_out_2[,2])
TP
```

The true negative rate (percentage of Nonviral that are correctly identified) is shown below.
```{r  }
# The true negative rate is 56%, which means 56% percentage of Nonviral that are correctly identified.
TN = confusion_out_2[1,1] / sum(confusion_out_2[,1])
TN
```

The false positive rate (percentage of Viral incorrectly identified as Nonviral) is shown below.
```{r  }
# There are 43% of Viral incorrectly identified as Nonviral 
FP = confusion_out_2[2,1] / sum(confusion_out_2[,1])
FP
```

The false negative rate (percentage of NonViral incorrectly identified as Viral) is shown below.
```{r  }
# There are 45% of NonViral incorrectly identified as Viral 
FN = confusion_out_2[1,2] / sum(confusion_out_2[,2])
FN

```

## Comparing Approach Performance
Averaged across 50 train-test splits, the KNN regression approach (regress first and threshold second) had an out-of-sample accuracy rate of around 0.552, whereas the KNN classification approach (threshold first and regress/classify second) had about 0.556 accuracy out-of-sample.

The KNN regression method has a smaller optimal K value where the error is lowest (K = 4), as opposed to the KNN classification method which has an optimal K that is much higher.