---
title: "saratoga-knn.Rmd"
author: "Jacob Rachiele"
date: "3/25/2020"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

## Saratoga House Prices

### Question

#### What is the best price-modeling strategy for predicting property market value?

### Data

The data set used to answer this question includes the prices of 1,728 houses in 
Saratoga County, New York in the year 2006. In addition to price, the data set includes
15 other attributes for each house --- 9 quantitative, e.g., house age, living area,
and number of bedrooms, and 6 categorical, e.g., whether the house has central air
conditioning, is a new construction, the type of heating system, and the type of fuel
used for that heating system.

### Methods

Two methods are used to analyze the data and form predicted house prices. The first was
linear regression, which assumes that house price is a linear function of the predictive
attributes, and the second is K-nearest-neighbors regression, which makes no assumptions
about the form of the function used to predict prices.

Instead, it predicts the price of a house based on the prices of other houses that are most
similar to the one being considered, where similarity is measured by how close the values
of the predictor attributes are to the house being considered. For example, if square
footage was the only predictor attribute and the house whose price we want to predict was
3,000 square feet, then we would find the average price of the K houses that are closest to
being 3,000 square feet and use that as our prediction.

80% of the houses were selected at random to train each model. The remaining 20% of the
houses were used to test the predictive performance of each model, measured by the root
mean squared error (RMSE) obtained my comparing each model's predicted prices with the actual prices contained in the test set.

#### Attribute Selection

The attributes used as predictors of housing prices were determined by examining the
correlation structure of the data, by visual examination of the relationships among
the predictors and prices, and ultimately by the ability of each potential predictor to reduce the RMSE of the predictions.

The attributes that were used to predict housing prices are given in Table 1

__Table 1__

| Attribute        | Description                                            |
| ---------        | --------------------------------------                 |
| living area      | living area in square feet                             |
| bedrooms         | The number of bedrooms in the house                    |
| bathrooms        | The number of bathrooms in the house                   |
| central air      | whether the house includes central air conditioning    |
| age              | The age of the house in years                          |
| percent college  | The percent of the neighborhood that graduated college |
| waterfront       | Whether the house is a waterfront property             |
| new construction | Whether the house was newly constructed                |

### Results

```{r include = FALSE}
# Load required libraries and define rmse function.
library(tidyverse)
library(mosaic)
library(splines)
library(FNN)
library(foreach)
data(SaratogaHouses)

rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

# Simple wrapper function to test for features that aren't factors.
# not.factor <- function(x) !is.factor(x)

# Show the correlation matrix to get an idea of price effects and interactions.
# cor(select_if(SaratogaHouses, not.factor))
```

```{r}
n <- nrow(SaratogaHouses)
n_train <- round(0.8*n)  # round to nearest integer
n_test <- n - n_train

set.seed(550)
rmse_vals <- do(250) * {
  train_cases <- sample.int(n, n_train, replace=FALSE)
  test_cases <- setdiff(1:n, train_cases)
  saratoga_train <- SaratogaHouses[train_cases,]
  saratoga_test <- SaratogaHouses[test_cases,]
  
  model <- lm(price ~
                livingArea * centralAir +
                livingArea * bathrooms +
                livingArea * bedrooms +
                waterfront + newConstruction +
                bs(age, df = 7) + bs(pctCollege, df = 7),
              data = saratoga_train)
  
  yhat_test <- predict(model, saratoga_test)
  
  rmse(saratoga_test$price, yhat_test)
  
}

k_grid <- exp(seq(log(1), log(100), length=33)) %>% round %>% unique

rmse_grid <- foreach(K = k_grid, .combine='c') %do% {
  out <- do(100) * {
    
    train_cases <- sample.int(n, n_train, replace=FALSE)
    test_cases <- setdiff(1:n, train_cases)
    saratoga_train <- SaratogaHouses[train_cases,]
    saratoga_test <- SaratogaHouses[test_cases,]
    
    training_features <- model.matrix(~ livingArea + centralAir +
                                        bathrooms + bedrooms +
                                        waterfront + newConstruction +
                                        pctCollege + age - 1,
                                      data = saratoga_train)
    test_features <- model.matrix(~ livingArea + centralAir +
                                    bathrooms + bedrooms +
                                    waterfront + newConstruction +
                                    pctCollege + age - 1,
                                  data = saratoga_test)
    
    training_response <- saratoga_train$price
    test_response <- saratoga_test$price
    
    training_scale <- apply(training_features, 2, sd)
    
    training_features <- scale(training_features, scale = training_scale)
    test_features <- scale(test_features, scale = training_scale)
    
    knn_model <- knn.reg(training_features, test_features, training_response, k = K)
    rmse(test_response, knn_model$pred)
  }
  mean(out$result)
}

lm_rmse <- mean(rmse_vals$result) %>% round
rmse_results <- data.frame(k = k_grid, k_rmse = rmse_grid, lm_rmse = lm_rmse)
best_k_index <- which.min(rmse_results$k_rmse)
best_k <- rmse_results$k[best_k_index]
best_rmse <- rmse_results$k_rmse[best_k_index] %>% round
```

The best performing K-nearest-neighbors model was at K = `r best_k` which had a prediction
RMSE of `r as.integer(best_rmse)`.

On the other hand, the linear regression model achieved a far superior prediction RMSE of 
`r as.integer(lm_rmse)`.

Figure 1 displays prediction RMSE against increasing values of K. The RMSE for 
K-nearest-neighbors is displayed in red. The RMSE of the linear regression model is 
the blue horizontal line.

```{r}
best_k_point <- data.frame(best_k = best_k, best_rmse = best_rmse)
rmse_subtitle <- "RMSE for KNN and Linear Regression Models"
lm_text <- "RMSE for linear regression model"
best_k_label <- paste("K =", best_k)

rmse_plot <- ggplot(data = rmse_results)
rmse_plot + geom_path(aes(x = k, y = k_rmse), color = "red", size = 0.75, alpha = 0.5) +
  geom_hline(yintercept = lm_rmse, color = "blue", size = 0.75, alpha = 0.5) +
  ggtitle("Figure 1", subtitle = rmse_subtitle) +
  xlab("K") +
  ylab("RMSE (Root Mean Squared Error)") +
  geom_point(data = best_k_point, aes(x = best_k, y = best_rmse),
             color = "black", alpha = 0.75, size = 2) +
  geom_text(data = best_k_point, aes(x = best_k, y = best_rmse, label = best_k_label),
            vjust = 1, hjust = -0.25, size = 3.5) +
  geom_text(data = rmse_results, aes(x = floor(max(k) / 2), y = lm_rmse,
            label = lm_text), vjust = -1, size = 4)
  
```

The greater predictive accuracy of the linear regresson model is the result of using the
right set of predictor attributes, modeling the interactions among those predictors, and
adding flexibility to the model through the use of the cubic spline, which divides the
range of the predictor variable into distinct intervals and fits a cubic polynomial curve
to each interval separately.

In particular, using age and percent of college graduates as simple predictors doesn't 
give the linear model much improvement in predictive power. However, using the cubic spline
mentioned above results in a dramatic improvement. Figures 2 and 3 below show the result
of fitting a cubic spline separately for both.

```{r}
age_model <- lm(price ~ bs(age, df = 7), data = SaratogaHouses)
age_subtitle <- "price vs age fitted with cubic spline"
xlabel <- "age of house (in years)"
p <- ggplot(data = SaratogaHouses)
p + geom_point(aes(x = age, y = price), color = "gray", alpha = 0.5) +
  geom_line(aes(age, age_model$fitted.values), color = "red", alpha = 0.75, size = 0.5) +
  ggtitle("Figure 2", subtitle = age_subtitle) +
  xlab(xlabel)

college_model <- lm(price ~ bs(pctCollege, df = 7), data = SaratogaHouses)
college_subtitle <- "price vs percent college graduates fitted with cubic spline"
xlabel <- "percentage of college graduates in neighborhood"
p +  geom_point(aes(x = pctCollege, y = price), color = "gray", alpha = 0.5) +
  geom_line(aes(pctCollege, college_model$fitted.values), 
            color = "red", alpha = 0.75, size= 0.5) +
  ggtitle("Figure 3", subtitle = college_subtitle) +
  xlab(xlabel)
```

Including interactions in the linear regression model provides a way to incorporate the
effect that one predictor variable has on another predictor variable into the model.
For example, living area is a powerful predictor of housing prices considered alone.
But by also considering the effect that living area has on prices in the context of the
values of other predictor variables, more accurate modeling and hence predictions are made.

For example, Figure 4 shows not only that larger living areas are associated
with higher house prices, but that, for houses with similar living areas, the presence of
central air conditioning may have a substantial effect on the house price. Considering
the joint effect of living area with both the number of bedrooms and the number of
bathrooms also boosts the predictive accuracy of the model.

```{r}
fig4_subtitle <- "price vs living area and central air conditioning"
p + geom_point(aes(x = livingArea, y = price, color = centralAir)) +
  ggtitle("Figure 4", subtitle = fig4_subtitle) +
  xlab("living area") +
  labs(color = "Central Air Conditioning")
```

Finally, as might be expected, including indicators for whether a house is a waterfront
property or is newly constructed also improves prediction accuracy. Figures 5 and 6 show
the relationship between each of those attributes and house prices.

```{r}
waterfront_subtitle <- "price distribution by whether house is on waterfront"
  p + geom_boxplot(aes(x = waterfront, y = price)) +
    ggtitle("Figure 5", subtitle = waterfront_subtitle) +
    xlab("is house a waterfront property?")

new_construction_subtitle <- "price distribution by whether house was newly constructed"
  p + geom_boxplot(aes(x = newConstruction, y = price)) +
    ggtitle("Figure 6", subtitle = new_construction_subtitle) +
    xlab("was house newly constructed?")
```

### Conclusion

The price of a house depends on many factors and relationships among those factors. Therefore a flexible modeling approach is necessary to give the most accurate predictions.

K-nearest-neighbors is considered a more flexible modeling strategy than linear
regression since it makes no assumptions about the underlying form of the relationship
between the response variable and the predictor variables, while linear regression does 
assume that a linear function can adequately model the response.

However, linear regression has two big advantages. One, it can be made substantially more
flexible by extending it with nonlinear techniques such as the cubic splines used on age
and percent of college graduates. Two, the interactions, i.e., context-specific effects
between variables can be made clear and explicit. Using linear regression gives a 
precise way to model how central air conditioning's effect on house price depends on
the living area of the house.

With all that said, if K-nearest-neighbors had outperformed linear regression in terms of
predictive accuracy, then that would be the recommended approach for determining house 
prices and setting tax rates. But that was not the case here. In fact, the linear
regression model signficantally outperformed K-nearest-neighbors and therefore is the
recommended approach for predicting house prices to help determine tax rates in Saratoga
County.

Specifically, the Saratoga County tax authority should predict prices using a linear
regression model using the following approach.

1. Include living area and consider it the most important determinant of house price.
2. Consider central air conditioning, the number of bedrooms, the number of bathrooms,
and make sure to include the interaction between each of those predictors and living area.
3. Include waterfront property and new constructions as simple predictors.
4. Include age of the house and percent of college graduates in the neighborhood as 
predictors. However, use cubic splines with each predictor. More specifically, divide 
each predictor into 5 intervals using the 20th, 40th, 60th, and 80th percentile. Then, on
each interval fit a cubic polynomial and join together the resulting curves across all
intervals to achieve one smooth, non-linear curve.
 
By following the approach outlined above the Saratoga County tax authority will obtain
very accurate predictions for house prices and be able to confidentally determine the
appropriate taxing strategy.